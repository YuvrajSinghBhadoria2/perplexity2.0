{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a204854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 36, 'total_tokens': 44, 'completion_time': 0.012850721, 'completion_tokens_details': None, 'prompt_time': 0.002492921, 'prompt_tokens_details': None, 'queue_time': 0.058004619, 'total_time': 0.015343642}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6d31-8e12-7761-81c6-1fdab69b53eb-0', usage_metadata={'input_tokens': 36, 'output_tokens': 8, 'total_tokens': 44})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import TypedDict,Annotated,Optional\n",
    "from langgraph.graph import add_messages,StateGraph,END\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from uuid import uuid4\n",
    "import json\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "model.invoke(\"hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db8eaa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = TavilySearchResults(max_results=4)\n",
    "\n",
    "tools = [search_tool]\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd18d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'xdx10mt6j', 'function': {'arguments': '{\"query\":\"current weather in bangalore\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 288, 'total_tokens': 310, 'completion_time': 0.033157069, 'completion_tokens_details': None, 'prompt_time': 0.016425003, 'prompt_tokens_details': None, 'queue_time': 0.054892797, 'total_time': 0.049582072}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6d34-a1b3-7c71-96cc-53510123a5b6-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in bangalore'}, 'id': 'xdx10mt6j', 'type': 'tool_call'}], usage_metadata={'input_tokens': 288, 'output_tokens': 22, 'total_tokens': 310})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools = model.bind_tools(tools=tools)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb89addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "async def model(state: State):\n",
    "    result = await llm_with_tools.ainvoke(state[\"messages\"])\n",
    "    return {\n",
    "        \"messages\": [result], \n",
    "    }\n",
    "\n",
    "async def tools_router(state: State):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "\n",
    "    if(hasattr(last_message, \"tool_calls\") and len(last_message.tool_calls) > 0):\n",
    "        return \"tool_node\"\n",
    "    else: \n",
    "        return END\n",
    "    \n",
    "async def tool_node(state):\n",
    "    \"\"\"Custom tool node that handles tool calls from the LLM.\"\"\"\n",
    "    # Get the tool calls from the last message\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    \n",
    "    # Initialize list to store tool messages\n",
    "    tool_messages = []\n",
    "    \n",
    "    # Process each tool call\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool_id = tool_call[\"id\"]\n",
    "        \n",
    "        # Handle the search tool\n",
    "        if tool_name == \"tavily_search_results_json\":\n",
    "            # Execute the search tool with the provided arguments\n",
    "            search_results = await search_tool.ainvoke(tool_args)\n",
    "            \n",
    "            # Create a ToolMessage for this result\n",
    "            tool_message = ToolMessage(\n",
    "                content=str(search_results),\n",
    "                tool_call_id=tool_id,\n",
    "                name=tool_name\n",
    "            )\n",
    "            \n",
    "            tool_messages.append(tool_message)\n",
    "    \n",
    "    # Add the tool messages to the state\n",
    "    return {\"messages\": tool_messages}\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"model\", model)\n",
    "graph_builder.add_node(\"tool_node\", tool_node)\n",
    "graph_builder.set_entry_point(\"model\")\n",
    "\n",
    "graph_builder.add_conditional_edges(\"model\", tools_router,[\"tool_node\", END],\n",
    ")\n",
    "graph_builder.add_edge(\"tool_node\", \"model\")\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c361bb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c5aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hi I am yuvi?', additional_kwargs={}, response_metadata={}, id='4eaca619-1294-46a9-ac09-80ffaea6568b'),\n",
       "  AIMessage(content=\"I'm happy to chat with you, Yuvi! Is there something on your mind that you'd like to talk about or ask about?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 284, 'total_tokens': 313, 'completion_time': 0.05930869, 'completion_tokens_details': None, 'prompt_time': 0.019075993, 'prompt_tokens_details': None, 'queue_time': 0.050954967, 'total_time': 0.078384683}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6d41-82b3-7002-9728-7a76557456ed-0', usage_metadata={'input_tokens': 284, 'output_tokens': 29, 'total_tokens': 313}),\n",
       "  HumanMessage(content='hi I am yuvi?', additional_kwargs={}, response_metadata={}, id='9d4e6f1d-3a12-4b23-9385-38996f81d2ae'),\n",
       "  AIMessage(content=\"It seems like you're just saying hello. Hello, Yuvi!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 328, 'total_tokens': 343, 'completion_time': 0.039678588, 'completion_tokens_details': None, 'prompt_time': 0.022462504, 'prompt_tokens_details': None, 'queue_time': 0.050160396, 'total_time': 0.062141092}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6d41-9762-7602-bd37-537f2c9f2058-0', usage_metadata={'input_tokens': 328, 'output_tokens': 15, 'total_tokens': 343}),\n",
       "  HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}, id='3402c9ca-95b5-4874-83fd-7c4d4d8902c8'),\n",
       "  AIMessage(content='Yuvi', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 357, 'total_tokens': 360, 'completion_time': 0.007363739, 'completion_tokens_details': None, 'prompt_time': 0.021173959, 'prompt_tokens_details': None, 'queue_time': 0.049996011, 'total_time': 0.028537698}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6d41-ddbb-7560-9cb4-d2b98a4684a3-0', usage_metadata={'input_tokens': 357, 'output_tokens': 3, 'total_tokens': 360}),\n",
       "  HumanMessage(content='when is the next space-x launch?', additional_kwargs={}, response_metadata={}, id='a7c5f8cc-fbd3-4351-8b18-29ea629778ec'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'x1ka4thkm', 'function': {'arguments': '{\"query\":\"next SpaceX launch date\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 377, 'total_tokens': 398, 'completion_time': 0.03635841, 'completion_tokens_details': None, 'prompt_time': 0.021278308, 'prompt_tokens_details': None, 'queue_time': 0.050239222, 'total_time': 0.057636718}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6d42-7580-7d91-b7a6-7cca557baae7-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'next SpaceX launch date'}, 'id': 'x1ka4thkm', 'type': 'tool_call'}], usage_metadata={'input_tokens': 377, 'output_tokens': 21, 'total_tokens': 398}),\n",
       "  ToolMessage(content='[{\\'title\\': \\'Rocket Launch Viewing Guide for Cape Canaveral\\', \\'url\\': \\'https://www.launchphotography.com/Launch_Viewing_Guide.html\\', \\'content\\': \"The next SpaceX Falcon 9 rocket will launch a Starlink batch from pad 40 on January 3 at 12:00-3:17 a.m. EST. A Falcon 9 will launch a Starlink batch from pad 40 on January 7 at 1:55-5:55 p.m. EST. Upcoming launches include more Starlink batches. A Falcon 9 will launch BlueBird 7 for AST SpaceMobile from pad 40 on January TBD.\\\\n\\\\nVULCAN & ATLAS V\\\\n\\\\nThe next United Launch Alliance Vulcan rocket will launch on TBD.\\\\n\\\\nNEW GLENN\\\\n\\\\nThe third flight of Blue Origin\\'s New Glenn rocket is  TBD.\", \\'score\\': 0.99549675}, {\\'title\\': \\'SpaceX (@SpaceX) / Posts / X - Twitter\\', \\'url\\': \\'https://x.com/SpaceX\\', \\'content\\': \\'The Starbase team plans to have the next Super Heavy booster stacked in December, which puts it on pace with the test schedule planned for the first Starship V3 vehicle and associated ground systems. Starshipâ€™s twelfth flight test remains targeted for the first quarter of 2026.\\\\n\\\\n304\\\\n\\\\n999\\\\n\\\\n7.7K\\\\n\\\\n1.3M\\\\n\\\\nImage 31: Square profile picture\\\\n\\\\nSpaceX\\\\n\\\\n@SpaceX\\\\n\\\\nÂ·\\\\n\\\\nDec 24\\\\n\\\\nStack complete\\\\n\\\\nImage 32: Image\\\\n\\\\n440\\\\n\\\\n1.6K\\\\n\\\\n11K\\\\n\\\\n999K\\\\n\\\\nNew to X?\\\\n\\\\nSign up now to get your own personalized timeline!\\', \\'score\\': 0.94208187}, {\\'title\\': \\'Launches - SpaceX\\', \\'url\\': \\'https://www.spacex.com/launches\\', \\'content\\': \\'# Upcoming launches\\\\n\\\\n| Mission | Vehicle | Launch site | Landing site | Launch Date And Time |\\\\n ---  --- \\\\n| COSMO-SkyMed Second Generation Mission | Falcon 9 | SLC-4E, California | Landing Zone | T-  0  1  2  0  1  2  3  4  5  6  7  8  9  :  0  1  2  3  4  5  0  1  2  3  4  5  6  7  8  9  :  0  1  2  3  4  5  0  1  2  3  4  5  6  7  8  9 |\\\\n| Starlink Mission | Falcon 9 | SLC-40, Florida | Droneship | January 3, 2026 00:00 - 03:17 ET |\\\\n\\\\n# Completed missions [...] | Crew-11 Mission | T+  0  1  2  3  4  5  6  7  8  9  0  1  2  3  4  5  6  7  8  9  0  1  2  3  4  5  6  7  8  9  D   0  1  2  0  1  2  3  4  5  6  7  8  9  :  0  1  2  3  4  5  0  1  2  3  4  5  6  7  8  9  :  0  1  2  3  4  5  0  1  2  3  4  5  6  7  8  9 | February 2026 | [...] | Mission | Time On-Orbit | Return Date And Time |\\\\n --- \\\\n| CRS-33 Mission | T+  0  1  2  3  4  5  6  7  8  9  0  1  2  3  4  5  6  7  8  9  0  1  2  3  4  5  6  7  8  9  D   0  1  2  0  1  2  3  4  5  6  7  8  9  :  0  1  2  3  4  5  0  1  2  3  4  5  6  7  8  9  :  0  1  2  3  4  5  0  1  2  3  4  5  6  7  8  9 | DECEMBER 2025 |\\', \\'score\\': 0.9079672}, {\\'title\\': \\'SpaceX Launch Schedule - RocketLaunch.org\\', \\'url\\': \\'https://rocketlaunch.org/launch-schedule/spacex\\', \\'content\\': \\'Updated: Dec 24, 7:30pm UTC\\\\n\\\\nBlueBird Block 2 #2\\\\n\\\\nImage 18: SpaceX logo\\\\nSpaceX\\\\n\\\\nTBD\\\\n\\\\nJan 2026\\\\n\\\\nðŸ“ Cape Canaveral Space Force Station\\\\n\\\\nðŸš€ Falcon 9 Block 5\\\\n\\\\nðŸŽ¯ Low Earth Orbit [...] Updated: Dec 9, 12:00am UTC\\\\n\\\\nSDA Tranche 1 Transport Layer F\\\\n\\\\nImage 26: SpaceX logo\\\\nSpaceX\\\\n\\\\nTBD\\\\n\\\\nMar 2026\\\\n\\\\nðŸ“ Vandenberg SFB\\\\n\\\\nðŸš€ Falcon 9 Block 5\\\\n\\\\nðŸŽ¯ Polar Orbit [...] Updated: Dec 8, 10:01pm UTC\\\\n\\\\nSDA Tranche 1 Transport Layer E\\\\n\\\\nImage 25: SpaceX logo\\\\nSpaceX\\\\n\\\\nTBD\\\\n\\\\nMar 2026\\\\n\\\\nðŸ“ Vandenberg SFB\\\\n\\\\nðŸš€ Falcon 9 Block 5\\\\n\\\\nðŸŽ¯ Polar Orbit\\', \\'score\\': 0.8740772}]', name='tavily_search_results_json', id='63c29405-626b-4372-88fd-3e4abd731539', tool_call_id='x1ka4thkm'),\n",
       "  AIMessage(content='The next SpaceX launch is scheduled for January 3, 2026, at 12:00-3:17 a.m. EST, launching a Starlink batch from pad 40 on Cape Canaveral.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 1838, 'total_tokens': 1883, 'completion_time': 0.050980626, 'completion_tokens_details': None, 'prompt_time': 0.133409198, 'prompt_tokens_details': None, 'queue_time': 0.065177152, 'total_time': 0.184389824}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6d42-7f8e-7b92-b55a-5c721c8e4808-0', usage_metadata={'input_tokens': 1838, 'output_tokens': 45, 'total_tokens': 1883}),\n",
       "  HumanMessage(content='Give me a 100 word essay on climate change?', additional_kwargs={}, response_metadata={}, id='4d824b27-6c4a-4cea-a0ad-0c6792f29628'),\n",
       "  AIMessage(content='Climate change is one of the most pressing issues of our time. Rising global temperatures are melting polar ice caps, causing sea levels to rise, and altering weather patterns. This leads to more frequent and severe heatwaves, droughts, and storms. The consequences are far-reaching, from devastating natural disasters to loss of biodiversity and food insecurity. Human activities, such as burning fossil fuels and deforestation, are significantly contributing to the problem. It is essential that we take immediate and sustained action to reduce greenhouse gas emissions and transition to renewable energy sources to mitigate the worst effects of climate change.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 117, 'prompt_tokens': 1903, 'total_tokens': 2020, 'completion_time': 0.14994115, 'completion_tokens_details': None, 'prompt_time': 0.117697612, 'prompt_tokens_details': None, 'queue_time': 0.051934198, 'total_time': 0.267638762}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6d51-122f-7590-9249-fe6bcfe6627b-0', usage_metadata={'input_tokens': 1903, 'output_tokens': 117, 'total_tokens': 2020})]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"configurable\":{\n",
    "        \"thread_id\":1\n",
    "    }\n",
    "}\n",
    "\n",
    "response = await graph.ainvoke({\n",
    "    \"messages\":[HumanMessage(content=\"when is the next space-x launch?\")]},config=config\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14401dc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIStatusError",
     "evalue": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k3wyrekvfn3sc70050vygff3` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6067, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      4\u001b[0m     }\n\u001b[1;32m      5\u001b[0m }\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mastream_events(\n\u001b[1;32m      8\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite a 50 words essay on climate changes?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]},\n\u001b[1;32m      9\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     10\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m ):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_chain_stream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m event\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     13\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:1514\u001b[0m, in \u001b[0;36mRunnable.astream_events\u001b[0;34m(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[0m\n\u001b[1;32m   1511\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclosing(event_stream):\n\u001b[0;32m-> 1514\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m event_stream:\n\u001b[1;32m   1515\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langchain_core/tracers/event_stream.py:1082\u001b[0m, in \u001b[0;36m_astream_events_implementation_v2\u001b[0;34m(runnable, value, config, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# Await it anyway, to run any cleanup code, and propagate any exceptions\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39msuppress(asyncio\u001b[38;5;241m.\u001b[39mCancelledError):\n\u001b[0;32m-> 1082\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langchain_core/tracers/event_stream.py:1037\u001b[0m, in \u001b[0;36m_astream_events_implementation_v2.<locals>.consume_astream\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# if astream also calls tap_output_aiter this will be a no-op\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclosing(runnable\u001b[38;5;241m.\u001b[39mastream(value, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[0;32m-> 1037\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m event_streamer\u001b[38;5;241m.\u001b[39mtap_output_aiter(run_id, stream):\n\u001b[1;32m   1038\u001b[0m             \u001b[38;5;66;03m# All the content will be picked up\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langchain_core/tracers/event_stream.py:192\u001b[0m, in \u001b[0;36m_AstreamEventsCallbackHandler.tap_output_aiter\u001b[0;34m(self, run_id, output)\u001b[0m\n\u001b[1;32m    190\u001b[0m tap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_tapped\u001b[38;5;241m.\u001b[39msetdefault(run_id, sentinel)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# wait for first chunk\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anext(output, sentinel)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first \u001b[38;5;129;01mis\u001b[39;00m sentinel:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langgraph/pregel/main.py:2971\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2969\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mamatch_cached_writes():\n\u001b[1;32m   2970\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2971\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   2972\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2973\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2974\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2975\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maaccept_push,\n\u001b[1;32m   2976\u001b[0m ):\n\u001b[1;32m   2977\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[1;32m   2979\u001b[0m         stream_mode,\n\u001b[1;32m   2980\u001b[0m         print_mode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2983\u001b[0m         asyncio\u001b[38;5;241m.\u001b[39mQueueEmpty,\n\u001b[1;32m   2984\u001b[0m     ):\n\u001b[1;32m   2985\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langgraph/pregel/_runner.py:304\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    302\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[1;32m    305\u001b[0m         t,\n\u001b[1;32m    306\u001b[0m         retry_policy,\n\u001b[1;32m    307\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream,\n\u001b[1;32m    308\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    309\u001b[0m             CONFIG_KEY_CALL: partial(\n\u001b[1;32m    310\u001b[0m                 _acall,\n\u001b[1;32m    311\u001b[0m                 weakref\u001b[38;5;241m.\u001b[39mref(t),\n\u001b[1;32m    312\u001b[0m                 stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream,\n\u001b[1;32m    313\u001b[0m                 retry_policy\u001b[38;5;241m=\u001b[39mretry_policy,\n\u001b[1;32m    314\u001b[0m                 futures\u001b[38;5;241m=\u001b[39mweakref\u001b[38;5;241m.\u001b[39mref(futures),\n\u001b[1;32m    315\u001b[0m                 schedule_task\u001b[38;5;241m=\u001b[39mschedule_task,\n\u001b[1;32m    316\u001b[0m                 submit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[1;32m    317\u001b[0m                 loop\u001b[38;5;241m=\u001b[39mloop,\n\u001b[1;32m    318\u001b[0m             ),\n\u001b[1;32m    319\u001b[0m         },\n\u001b[1;32m    320\u001b[0m     )\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langgraph/pregel/_retry.py:132\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mastream(task\u001b[38;5;241m.\u001b[39minput, config):\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:871\u001b[0m, in \u001b[0;36mRunnableSeq.astream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m             aiterator \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mtap_output_aiter(\n\u001b[1;32m    868\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mrun_id, aiterator\n\u001b[1;32m    869\u001b[0m             )\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# consume into final output\u001b[39;00m\n\u001b[0;32m--> 871\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _consume_aiter(aiterator)\n\u001b[1;32m    872\u001b[0m \u001b[38;5;66;03m# sequence doesn't emit output, yield to mark as generator\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:904\u001b[0m, in \u001b[0;36m_consume_aiter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    902\u001b[0m output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    903\u001b[0m add_supported \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 904\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;66;03m# collect final output\u001b[39;00m\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m add_supported:\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langchain_core/tracers/event_stream.py:192\u001b[0m, in \u001b[0;36m_AstreamEventsCallbackHandler.tap_output_aiter\u001b[0;34m(self, run_id, output)\u001b[0m\n\u001b[1;32m    190\u001b[0m tap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_tapped\u001b[38;5;241m.\u001b[39msetdefault(run_id, sentinel)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# wait for first chunk\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anext(output, sentinel)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first \u001b[38;5;129;01mis\u001b[39;00m sentinel:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:1587\u001b[0m, in \u001b[0;36mRunnable.atransform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m final: Input\n\u001b[1;32m   1585\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1587\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[1;32m   1588\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[1;32m   1595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[1;32m   1596\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:1587\u001b[0m, in \u001b[0;36mRunnable.atransform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m final: Input\n\u001b[1;32m   1585\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1587\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[1;32m   1588\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[1;32m   1595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[1;32m   1596\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:1168\u001b[0m, in \u001b[0;36mRunnable.astream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mastream\u001b[39m(\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   1152\u001b[0m     config: RunnableConfig \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Output]:\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Default implementation of `astream`, which calls `ainvoke`.\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m    Subclasses must override this method if they support streaming output.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \n\u001b[1;32m   1167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1168\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:473\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmodel\u001b[39m(state: State):\n\u001b[0;32m----> 7\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m llm_with_tools\u001b[38;5;241m.\u001b[39mainvoke(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [result], \n\u001b[1;32m     10\u001b[0m     }\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:5570\u001b[0m, in \u001b[0;36mRunnableBindingBase.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5563\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5564\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m   5565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5568\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5569\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   5571\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   5572\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   5573\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   5574\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:421\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    419\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AIMessage:\n\u001b[1;32m    420\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 421\u001b[0m     llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m    422\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    423\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    424\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    425\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    426\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    427\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    428\u001b[0m         run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m, cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    433\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1128\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1126\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1127\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m   1129\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1130\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1086\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m   1074\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m   1075\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m   1076\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1084\u001b[0m             ]\n\u001b[1;32m   1085\u001b[0m         )\n\u001b[0;32m-> 1086\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1087\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1088\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m   1090\u001b[0m ]\n\u001b[1;32m   1091\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1339\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1339\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[1;32m   1340\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1341\u001b[0m     )\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/langchain_groq/chat_models.py:614\u001b[0m, in \u001b[0;36mChatGroq._agenerate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    610\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    613\u001b[0m }\n\u001b[0;32m--> 614\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, params)\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/groq/resources/chat/completions.py:941\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    780\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[1;32m    781\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m    782\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 941\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    943\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m    944\u001b[0m             {\n\u001b[1;32m    945\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    946\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    947\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcitation_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: citation_options,\n\u001b[1;32m    948\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompound_custom\u001b[39m\u001b[38;5;124m\"\u001b[39m: compound_custom,\n\u001b[1;32m    949\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_tool_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m: disable_tool_validation,\n\u001b[1;32m    950\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m: documents,\n\u001b[1;32m    951\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: exclude_domains,\n\u001b[1;32m    952\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    953\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    954\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    955\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_domains,\n\u001b[1;32m    956\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_reasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_reasoning,\n\u001b[1;32m    957\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    958\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    959\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m    960\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    961\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    962\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    963\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    964\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    965\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m    966\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_format,\n\u001b[1;32m    967\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    968\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m: search_settings,\n\u001b[1;32m    969\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    970\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    971\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    972\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m    973\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    974\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    975\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    976\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    977\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    978\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    979\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    980\u001b[0m             },\n\u001b[1;32m    981\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    982\u001b[0m         ),\n\u001b[1;32m    983\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    984\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    985\u001b[0m         ),\n\u001b[1;32m    986\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    987\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    988\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m    989\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/groq/_base_client.py:1762\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1750\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1757\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1758\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1759\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1760\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1761\u001b[0m     )\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/Documents/perplexity 2.0/Server/venv/lib/python3.10/site-packages/groq/_base_client.py:1576\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1573\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1575\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1576\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1578\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAPIStatusError\u001b[0m: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k3wyrekvfn3sc70050vygff3` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6067, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"configurable\":{\n",
    "        \"thread_id\":5\n",
    "    }\n",
    "}\n",
    "\n",
    "async for event in graph.astream_events(\n",
    "    {\"messages\": [HumanMessage(content=\"write a 50 words essay on climate changes?\")]},\n",
    "    config=config,\n",
    "    version=\"v2\",\n",
    "):\n",
    "    if event[\"event\"] == \"on_chain_stream\" and event.get(\"name\") == \"model\":\n",
    "        chunk = event[\"data\"][\"chunk\"]\n",
    "        if \"messages\" in chunk:\n",
    "            print(chunk[\"messages\"][-1].content, end=\"\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce96830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
